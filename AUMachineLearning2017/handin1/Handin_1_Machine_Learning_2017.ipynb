{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style='font-size: 14px;'>Table of Content</b><ol><li>Introduction: OCR and Logistic Regression</li><li>Loading and visualizing the data</li><li>Part I: Logistic Regression</li><ol><li>Likelihood</li><li>Gradient descent</li><ol><li>(Batch) Gradient Descent</li><li>Stochastic gradient descent</li><li>Minibatch stochastic gradient descent</li></ol><li>Deliverables</li><ol><li>Report</li><li>Theoretical Questions</li></ol></ol><li>Part II: Multinomial/Softmax  Regression</li><ol><li>Numerical Issues with Softmax</li><li>Deliverables</li><ol><li>Report</li><li>Theoretical Question(s):</li></ol></ol><li>Uploading to BlackBoard</li></ol>\n",
    "<hr>\n",
    "\n",
    "Report length is 5 pages. You are allowed to be up to 3 members in a group.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction: OCR and Logistic Regression\n",
    "In this handin you will build classifiers for optical character recognition (OCR) using **Logistic Regression** and **Softmax**. The classifier will take images of numbers as input and output which number the image represent. The input images will look like those below for which the classifier (hopefully) will predict the numbers 5041.\n",
    "\n",
    "   <img src=\"https://tensorflow.rstudio.com/images/MNIST.png\" style=\"width:200px;\" /> \n",
    "\n",
    "The handin is split into two parts. Each part uses a different model for learning. The first considers *logistic regression* and the second considers *softmax*. These will both be explained later. \n",
    "\n",
    "We have compiled a list of tips, tricks and common coding mistakes in the <a href=\"https://blackboard.au.dk/webapps/discussionboard/do/message?action=list_messages&course_id=_66530_1&nav=discussion_board_entry&conf_id=_121247_1&forum_id=_92703_1&message_id=_146625_1\">discussion board</a>. \n",
    "\n",
    "The performance of your final classifier is easy to compare. This year we made a highscore board where you can see the scores of your fellow students. This is only for fun and will not be a part of judging your handin, but we encourage you to try and beat the TA classifier ;-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500\"\n",
       "            height=\"220\"\n",
       "            src=\"https://cs.au.dk/~alexmath/highscore.php?border=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f74f8282da0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://cs.au.dk/~alexmath/highscore.php', width=500, height=220, border=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Loading and visualizing the data\n",
    "We will use the AU-Digits data set generated by previous Machine Learning students here at Aarhus University. \n",
    "The following code loads the data into two sets: the *training* and the *test* set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "\n",
    "def load_train_data():\n",
    "    filename = \"auTrain.npz\"\n",
    "    if not os.path.exists(filename):\n",
    "        #os.system('wget https://users-cs.au.dk/jallan/ml/data/auTrain.npz')\n",
    "        with open(filename,'wb') as fh:\n",
    "            fh.write(urllib.request.urlopen(\"https://users-cs.au.dk/jallan/ml/data/%s\" % filename).read())\n",
    "\n",
    "    tmp = np.load(filename)\n",
    "    return tmp['digits'], tmp['labels']\n",
    "\n",
    "def load_test_data():\n",
    "    filename = \"auTest.npz\"\n",
    "    if not os.path.exists(filename):\n",
    "        #os.system('wget https://users-cs.au.dk/jallan/ml/data/auTest.npz')\n",
    "        with open(filename,'wb') as fh:\n",
    "            fh.write(urllib.request.urlopen(\"https://users-cs.au.dk/jallan/ml/data/%s\" % filename).read())\n",
    "        \n",
    "    tmp = np.load(filename)\n",
    "    return tmp['digits'], tmp['labels']\n",
    "\n",
    "images, labels = load_train_data()\n",
    "images_test, labels_test = load_test_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now loaded. The following code visualizes some of the images. You might find this handy later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAjCAYAAAA319myAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngQMHDl4iDqPrwIEDBy+R/wO8ekiTqeEOxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f74f82372b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAjCAYAAAA319myAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhhtu/Iv4f0xSxw/1olM7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f74c080a470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the first 16 digits from training set\n",
    "x = images[0:16, :].reshape(-1, 28, 28)\n",
    "x = x.transpose(1, 0, 2)\n",
    "plt.imshow(x.reshape(28, -1), cmap='bone')\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.figure()\n",
    "\n",
    "# Visualize the first 16 two's from training set\n",
    "# HINT: You might want to try to visualize the first 10 seven's. \n",
    "idx1 = (labels == 2)\n",
    "img2s = images[idx1, :]\n",
    "x2 = img2s[0:16, :].reshape(-1, 28, 28)\n",
    "x2 = x2.transpose(1, 0, 2)\n",
    "plt.imshow(x2.reshape(28, -1), cmap='bone')\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Part I: Logistic Regression\n",
    "In supervised machine learning the goal is to learn a target function $f: X\\rightarrow Y$. If we model $f$ deterministically each $x$ has exactly one correct label $y$ which is given by $y=f(x)$. In Logistic Regression things are a bit different. Here, we model the the target function as a probability distribution $f(x)=\\Pr(y\\mid x)$. Therefore each $x$ does not have one correct label, each $x$ has a corresponding probability for each $y$. \n",
    "\n",
    "<div style=\"border: 1px solid black; margin:16px; padding: 16px; \"><b>Example</b>: \n",
    "Given an image $x$ like the following\n",
    "\n",
    "<img src=\"http://users-cs.au.dk/jallan/ml/exercises/handin1/figs/twoorseven.png\"><br>\n",
    "\n",
    "It is not entirely clear if this image is a 2 or a 7. In Logistic Regression $f(x)$ is a probability distribution and we might thus get something like <br><br>\n",
    "\n",
    "  $\\quad\\quad f(x)=\\Pr(y\\mid x)=[0.05, \\;0.00,\\; 0.40,\\; 0.02,\\; 0.01,\\; 0.00,\\; 0.03,\\; 0.42,\\; 0.02,\\; 0.05]$<br><br>\n",
    "\n",
    "This means that the probability of $x$ being a 2 is 40%, the probability of 7 is 42% and so on. This stands in contrast to just outputting 7. </div> \n",
    "\n",
    "To this end we use the logistic function\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}\\quad\\quad\\text{where}\\quad\\quad z = \\sum_{i=0}^d \\theta_i x_i = \\theta^\\intercal x$$\n",
    "\n",
    "You need to notice two things here: \n",
    "   1. $z$ is a linear combination of the input and the vector $\\theta$. The goal of learning is to find good values for $\\theta$.\n",
    "   2. The bias variable is encoded into the input vectors as $x_0=1$ on all data points (this is similar to what we did in lectures and previous exercises, if this confuses you re-read pages 5-7). \n",
    "\n",
    "The following code plots the logistic function $\\sigma(z)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHmZJREFUeJzt3XuYVNWZ7/Hvy11REVFBEJTbRI0oGAVUoj0mCo4X0BgF\n1KCgQ6IkZmJOkCSjHZ1zRjMjegAvQTkEiVyiiBe8YRyaiEHFRwQRWkCUO6jgDRBout/zx6qGou2m\nq7ura1ft+n2eZz+1d9WuXS9F8+vF2muvbe6OiIjES4OoCxARkfRTuIuIxJDCXUQkhhTuIiIxpHAX\nEYkhhbuISAwp3CVrmdlgM3uplu9dYmbn1OJ9l5nZGjP7ysxOrc1n10Zd/qwilTGNc5d0MLOPgGHu\n/j8RfPZEYK27356GY60Efunus+peWZWfcRzwEdDI3cvq63Mkv6nlLrK/44Cl9fwZBnjiUaReKNyl\n3pnZjWa2wsw+M7OnzeyYpNcuMLNiM/vczB4wsyIzG5p4bYiZvZa0731mttnMvjSzRWZ2kpndCFwN\n/CbRlfJMYt+PzOy8xHoDM/utma1MvHeBmbWrUGMTM/ua8G9isZmtSDxfZmadkvabaGZ3JtbPNbO1\nZvarRF3rzey6pH2bmdm9ZvaxmX1hZn83s2bA3MQuXyRq7lXJn/UsM3sr8b28aWZnJr02x8zuNLN5\nife/ZGZH1PGvSWJG4S71KhGw/we4AjgGWANMS7x2JPAEMBJoBXwAnFnhEJ7Y9wKgD9DF3VsAVwJb\n3P0R4HHgj+5+mLv3r6SMW4GrgH6J9w4Fduz3Ie673f1QQmu6m7t3Tf78A2gDHAq0BW4AHjCzFonX\n7gV6AL2BlsBvgFKg/FzAYYma36zwZ20JzALuT3wv9wHPJ54vNwgYAhwFNAV+XU2dkmcU7lLfBgMT\n3H2Ru5cAo4DeZtYBuBBY4u7PuHuZu48BNldxnBJCiJ5kZubuH7h7VftWNAz4nbuvBHD399z98wPs\nb1WsV2Y3cJe7l7r7i8A24DtmZsD1wC/cfZMHbyS+g+qOfRGw3N2nJL6XaUAxcEnSPhPd/UN33wX8\nFeheTZ2SZxTuUt/aAqvLN9x9O7AVaJd4bW2F/ddVdhB3nwOMAx4ANpvZw2Z2SIo1tAdW1bDuVG2p\ncFJ0B3AIcCShRV2bz93vO0tYTfjOym2q5DNF9lK4S33bQDhJCYCZNSd0NawHNhKCN9mxVR3I3ce5\n++nAScB3gP9V/lI1NawFOtes7L12AAcnbbdJ8X2fATur+Nzq6t0AHF/huQ6E70wkJQp3SacmZtY0\naWkITAWuN7NTzKwpof/9DXdfAzwPnGxml5pZQzMbAbSu7MBmdrqZ9TSzRsA3hOAsbzFvBjpV9r6E\nR4G7zKxL4ljdKvRfH8hCYHDipGw/4NxU3uRhjPFEYLSZHZN4f28zawx8mqi9ql84LwBdzWxg4nu5\nCjgReC7FmkUU7pJWzxNaut8kHu9w91eBfweeIrQ8OwIDAdx9C/Bj4L8ILd0TgLeBXZUc+zDgEUKX\nzkeJ/f8r8doE4LtmttXMnko8l9w6Hk3ol55tZl8Swv6gKv4MFVvVvwQuBT4nnMScecBvYP/3/xp4\nD1gAbAHuBhq4+zfA/wZeT9Tcc78DuG8FLk68/7PE40VJ5wl0cYpUSxcxSdZInIRcBwx297nV7S8i\nVVPLXSKVGOfeItFl87vE029EWZNIHCjcJWpnAh8CnxCGAPZPDO8TkTpQt4yISAyp5S4iEkONMvlh\nZqb/JoiI1IK712iiuYy33N1dS5qWO+64I/Ia4rTo+9R3ma1LbahbRkQkhhTuIiIxpHDPYQUFBVGX\nECv6PtNH32X0MjoUMszUqnOqIiI1YWZ4tp9QFRGR+qdwFxGJIYW7iEgMKdxFRGJI4S4iEkMKdxGR\nGFK4i4jEkMJdRCSGqg13M5tgZpvNbPEB9hljZivM7F0z657eEkVEpKZSablPBPpW9aKZXQh0dveu\nwHDg4TTVJiIitVRtuLv7PMKd36vSH3gsse+bQAsza52e8kREpDbScbOOdsDapO31iec2p+HYIhJH\ne/bAtm2wfXtYtm2DHTtg9+6w7NpV+XrydkkJlJZCWdn+jzVdLysD930LVP6YyefSIKN3YgIoLCzc\nu15QUKDZ40Ry2e7dsHEjbNgA69fDJ5/A1q1h+fzz/de/+mpfkO/eHXXlWa0osdRFSrNCmtlxwHPu\nfkolrz0MzHH36YntYuBcd/9Wy12zQorkmD174OOPYcWKfcuqVSHIN2yATz+t3XEbNIDmzeGQQ8Jj\n8+Zw8MHQtGlYmjQJS/J6xe3GjaFhw3Cshg1rv16+AJiFpXy94mMmn0tiPXvWeFbIVFvullgq8yxw\nMzDdzHoDX1QW7CKS5b7+GhYtgnfegYULw/L++yHgq9KwIbRpA+3aQdu20Lo1tGoFLVvCEUeEpWXL\nsLRosS/QmzatNMQkfaoNdzObAhQArcxsDXAH0ARwdx/v7i+Y2b+Y2UpgO3B9fRYsImny2Wcwd25Y\n/v53WLy48j7f9u2ha9d9S5cucOyxIdCPOioEvGQd3axDJF+4w5IlMGtWWObP3z/MGzeGk0+G006D\nHj3C4ymnhNa2RKo2N+vI+AlVEcmwNWtgyhT4y19CN0u5Jk2gTx8499yw9OoFzZpFV6eklcJdJI7K\nyuCll2DMGHj55X3Pt2oF/fvDxRfDD38Ihx4aXY1SrxTuInGyaxdMmAD33x9GtkBojffvD9dcA337\nhu4XiT2Fu0gclJTApElw552wNnFNYYcOMGIEDBsWRq1IXlG4i+S6WbPg3/4NVq4M2926we23w4AB\n0Ej/xPOV/uZFctW6dXDLLfDUU2G7a1f4wx/gqqv2XZQjeUvhLpJr3OGhh2DkyHApf/PmcNdd8POf\nq6Uue+knQSSXfP556EOfOTNsDxgQRsS0bx9tXZJ1FO4iuWL+fBg0CFavhsMOg/HjQxeMSCUU7iK5\nYNIkuOGGMM/LGWfAtGnQqVPUVUkW01kXkWzmHoY3XnddCPZbboF58xTsUi213EWyVUkJDB8OEyeG\n0S9jxsDNN0ddleQIhbtINiopCf3pM2fCQQeFbphLL426KskhCneRbLNnD1x7bQj2ww8Pc8T06hV1\nVZJjFO4i2aS0FIYOhenTw6ReL78MPXtGXZXkIJ1QFckW7nDTTTB5crgw6cUXFexSawp3kWwxenQY\nu96sWZgv5uyzo65IcpjuxCSSDV54Icyx7g5//Sv8+MdRVyRZpDZ3YlLLXSRq778PAweGYC8sVLBL\nWqjlLhKlrVvDFaerVsGVV4Yhj1ajBprkAbXcRXKJO9x4Ywj2730vXKykYJc0UbiLRGXChDAX+6GH\nwhNPwMEHR12RxIjCXSQKxcVhnhgIc7N37BhtPRI7CneRTNu1CwYPhh07wk2rr7466ookhhTuIpl2\nxx2wcGForT/wQNTVSExptIxIJi1aFE6elpXB66/DmWdGXZHkAI2WEclmpaVhCt/S0jB1r4Jd6pFa\n7iKZ8uCDIdTbtoVly8Kt8kRSUG8tdzPrZ2bFZrbczEZW8vphZvasmb1rZu+Z2XU1KUIk9jZsgFGj\nwvrYsQp2qXfVhruZNQDGAX2B7wKDzOyECrvdDLzv7t2BfwbuNTNNJyxS7pe/hK++gksugcsui7oa\nyQOptNx7AivcfbW7lwDTgP4V9nHg0MT6ocAWd9+TvjJFctjcueEipebNYdw4XYUqGZFKuLcD1iZt\nr0s8l2wccJKZbQAWAbekpzyRHFdWBr/+dVgfORI6dIi2Hskb6eo66QssdPfzzKwz8IqZneLu2yru\nWFhYuHe9oKCAgoKCNJUgkoWmT4e334ZjjoFf/SrqaiRHFBUVUVRUVKdjVDtaxsx6A4Xu3i+xfRvg\n7n5P0j6zgP9099cT268CI9397QrH0mgZyR87d8IJJ8Dq1WEemaFDo65IclR9jZZZAHQxs+PMrAkw\nEHi2wj6rgR8mimgN/BOwqiaFiMTO2LEh2Lt1gyFDoq5G8kxK49zNrB/wfwm/DCa4+91mNpzQgh9v\nZscAfwaOSbzlP919aiXHUctd8sOWLdC5M3z5Jbz0EvTtG3VFksNq03LXRUwi9WHUKLj7bjj/fJg9\nO+pqJMcp3EWywZYtcPzxsG0bvPkm9OwZdUWS4zS3jEg2GD06BHu/fgp2iYxa7iLptHVraLV//TX8\n4x+aHEzSQi13kajdf38I9vPPV7BLpNRyF0mXL76A444Lc8i89hr06RN1RRITarmLRGnMmBDs552n\nYJfIqeUukg47doR5Y7ZsgTlzQNNqSBqp5S4SlcceC8F++ulw7rlRVyOicBeps7IyuO++sH7rrZrS\nV7KCwl2krmbNguXLQ7fMFVdEXY0IoHAXqbt77w2Pt9wCjXQDMskOOqEqUhdvvw1nnBHuibp2re6N\nKvVCJ1RFMm306PB4440KdskqarmL1Na6dWGqAYBVq3QLPak3armLZNIjj0BpKVx2mYJdso5a7iK1\nUVISAn3TJl20JPVOLXeRTHn66RDsJ56oi5YkKyncRWrjwQfD40036aIlyUrqlhGpqWXL4KSToHlz\nWL8eWrSIuiKJOXXLiGTCQw+Fx2uuUbBL1lLLXaQmtm2Ddu3C1L7vvgunnhp1RZIH1HIXqW9Tp4Zg\nP/tsBbtkNYW7SE08+mh4HD482jpEqqFuGZFULV4cWustWsDGjXDQQVFXJHlC3TIi9WnChPB49dUK\ndsl6armLpGLnznAidetWeOcd6NEj6ookj6jlLlJfnn46BHuPHgp2yQkKd5FUlHfJDBsWbR0iKUop\n3M2sn5kVm9lyMxtZxT4FZrbQzJaY2Zz0likSoY8+gr/9DZo1g8GDo65GJCXV3hPMzBoA44AfABuA\nBWb2jLsXJ+3TAngAuMDd15vZkfVVsEjGTZwYHn/0I2jZMtpaRFKUSsu9J7DC3Ve7ewkwDehfYZ/B\nwAx3Xw/g7p+lt0yRiJSVwZ//HNbVJSM5JJVwbwesTdpel3gu2T8BR5jZHDNbYGbXpqtAkUjNnRvu\njXr88ZraV3JKum7V3gg4DTgPaA7MN7P57r4yTccXicZjj4XHa66BBhp/ILkjlXBfDyTfQ+zYxHPJ\n1gGfuftOYKeZ/R04FfhWuBcWFu5dLygooEB3sJFstWMHPPlkWL9W/xmVzCkqKqKoqKhOx6j2IiYz\nawh8QDihuhF4Cxjk7suS9jkBGAv0A5oCbwJXufvSCsfSRUySO6ZODaNjevWCN96IuhrJY7W5iKna\nlru7l5rZCGA2oY9+grsvM7Ph4WUf7+7FZvYysBgoBcZXDHaRnFPeJaNWu+QgTT8gUplNm8J0Aw0a\nhEnCjtToXomOph8QSZepU8MwyIsuUrBLTlK4i1RGXTKS49QtI1LRe+/BKafA4YeH7pmmTaOuSPKc\numVE0mHy5PB41VUKdslZarmLJCsthQ4dYMMGmDcv3CtVJGJquYvU1Zw5Idg7dYKzzoq6GpFaU7iL\nJEs+kWo1aiiJZBV1y4iU274dWrcOjytWQJcuUVckAqhbRqRuZs4MwX7WWQp2yXkKd5FyGtsuMaJu\nGRGA9evDKJlGjcJ0A0ccEXVFInupW0aktqZMCdMNXHyxgl1iQeEuAvsuXFKXjMSEumVEFi2C7t1D\ni33jRmjSJOqKRPajbhmR2ig/kTpwoIJdYkMtd8lve/ZA+/ZhgrD586F376grEvkWtdxFaurVV0Ow\nd+0abqcnEhMKd8lvmm5AYkrdMpK/vv46TDfwzTewahV07Bh1RSKVUreMSE3MmBGCvU8fBbvEjsJd\n8lf52Paf/CTaOkTqgbplJD+tWQPHHx+GPm7aFG6pJ5Kl1C0jkqrHHwd36N9fwS6xpHCX/OMOkyaF\ndXXJSEypW0byz1tvhTHtRx8dZoNs1CjqikQOSN0yIqkob7VffbWCXWJLLXfJL7t2Qdu2sHUrLFwY\nJgwTyXJquYtU5/nnQ7CfcoqCXWItpXA3s35mVmxmy81s5AH2O8PMSszs8vSVKJJG5dMN6ESqxFy1\n3TJm1gBYDvwA2AAsAAa6e3El+70CfAP8P3d/qpJjqVtGovPZZ3DMMeGOS+vXQ5s2UVckkpL66pbp\nCaxw99XuXgJMA/pXst/PgSeBT2pSgEjGTJ0apvjt21fBLrGXSri3A9Ymba9LPLeXmbUFBrj7Q4Cm\n1pPsVN4lM2RItHWIZEC6TqjeDyT3xSvgJbssXQpvvw2HHQaXXhp1NSL1LpVBvuuBDknbxyaeS3Y6\nMM3MDDgSuNDMStz92YoHKyws3LteUFBAQUFBDUsWqYXyVvuVV8JBB0Vbi0g1ioqKKCoqqtMxUjmh\n2hD4gHBCdSPwFjDI3ZdVsf9E4DmdUJWsUVoKxx0XTqK+9lqY4lckh9TmhGq1LXd3LzWzEcBsQjfO\nBHdfZmbDw8s+vuJbalKASL17+eUQ7J07w9lnR12NSEakdO21u78EfKfCc3+qYt+haahLJH0mTAiP\nw4bpVnqSNzT9gMTb5s1w7LFhbPvatWHqAZEco+kHRCqaPDmMbb/oIgW75BWFu8SXOzz6aFgfNiza\nWkQyTN0yEl+vvx5GxrRpE26r17hx1BWJ1Iq6ZUSSlbfahwxRsEveUctd4umrr8IkYTt2wPLl0LVr\n1BWJ1Jpa7iLlpkwJwX7OOQp2yUsKd4kfd3jwwbD+059GW4tIRNQtI/FTfiL16KPDidSmTaOuSKRO\n1C0jAvta7TfcoGCXvKWWu8TLJ5+EK1JLS2HVqjBhmEiOU8tdZMIEKCmBiy9WsEteU7hLfJSWwsMP\nh/Wf/SzaWkQipnCX+HjxxXACtVMnuOCCqKsRiZTCXeJj7Njw+LOfQQP9aEt+0wlViYclS6BbNzj4\n4DC17xFHRF2RSNrohKrkr9Gjw+PQoQp2EdRylzjYtCmMjCkpgRUrwu30RGJELXfJT+PGwe7dMGCA\ngl0kQS13yW3bt0OHDrB1K8ybpxtgSyyp5S75Z9KkEOy9esFZZ0VdjUjWULhL7iothfvuC+u33gpW\no4aNSKwp3CV3PfEErFwJHTvCZZdFXY1IVlG4S24qK4O77grro0ZBo0bR1iOSZRTukptmzIClS8PJ\n1CFDoq5GJOso3CX3lJXBnXeG9VGjoEmTaOsRyUIaCim5Z8YMuOKKMG/7ypW6IYfEnoZCSvxVbLUr\n2EUqpZa75JaZM+Hyy6FtW/jwQ2jWLOqKROpdvbXczayfmRWb2XIzG1nJ64PNbFFimWdm3WpShEhK\nSkrgttvC+m9/q2AXOYBqw93MGgDjgL7Ad4FBZnZChd1WAee4+6nAfwCPpLtQEcaPh+XLoWtX+Nd/\njboakayWSsu9J7DC3Ve7ewkwDeifvIO7v+HuXyY23wDapbdMyXtffQV/+ENYv+ceaNw42npEslwq\n4d4OWJu0vY4Dh/cNwIt1KUrkW+65Bz79NEwMNmBA1NWIZL20XtZnZv8MXA/0qWqfwsLCvesFBQUU\nFBSkswSJo7Vr992M4957NYeMxF5RURFFRUV1Oka1o2XMrDdQ6O79Etu3Ae7u91TY7xRgBtDP3T+s\n4lgaLSM195OfwOTJcOWVMH161NWIZFxtRsukEu4NgQ+AHwAbgbeAQe6+LGmfDsCrwLXu/sYBjqVw\nl5qZMwfOOy+MZ1+6FDp1iroikYyrTbhX2y3j7qVmNgKYTeijn+Duy8xseHjZxwP/DhwBPGhmBpS4\ne8+a/xFEkuzcCT/9aVj//e8V7CI1oIuYJHsVFoYRMieeCAsX6mpUyVv10i2TTgp3SVlxMZx6arg3\n6ty5cM45UVckEhnNLSPxUFYWumN274ZhwxTsIrWgcJfsM2ZMaK0fdRT88Y9RVyOSk9QtI9nl3XfD\nza5374anntLt80RQt4zkuh07YPDgEOzDhyvYRepA4S7Z49ZbYdkyOOGEfVekikitqFtGssNTT8GP\nfhRumffmm9C9e9QViWQNdctIblq0KEwxAGGCMAW7SJ2p5S7R2rwZevaENWvg2mth0iRNDCZSgS5i\nktyya1eYN+Yf/4DevcM8Mrq7ksi3qFtGckdZGdxwQwj29u3DvVEV7CJpo3CXzHOHm26Cv/wFDj4Y\nnnkG2rSJuiqRWFG4S2a5wy23wJ/+FFrqzz0HPXpEXZVI7CjcJXPc4Te/gbFjw5DHp58Ofe4iknZp\nvc2eSJX27IGbb4bx46FRI3jySejbN+qqRGJL4S71b9s2uOoqeOGF0BUzdSpccknUVYnEmsJd6tfG\njXDxxfDOO9CqVehjP/PMqKsSiT31uUv9mT07XG36zjvQuTPMn69gF8kQhbukX0kJjBoV+tQ/+SSc\nNJ0/H7p2jboykbyhcJf0WrwY+vSBu++GBg3grrtCC/6oo6KuTCSvqM9d0mP79nBD6/vug9JSOPZY\nmDIFvv/9qCsTyUtquUvdlJbCY4/BSSfBf/93mFZgxAhYskTBLhIhtdyldsrKYMYMuP12KC4Oz512\nWrjy9PTTo61NRBTuUkPbtsHkyeEm1uWhfvzxoUvmmmugYcMoqxORBIW7VM893Lj6scdg4kT48svw\nfLt28Pvfw9ChYToBEckaCnepnHtomc+cCY8/DkuX7nvtrLPgF7+Ayy+Hxo2jq1FEqqRwl322bg3z\nq7/yCsyaBatW7XutVSsYOBCuu0596iI5QOGer0pLYeVKWLAA5s0Ly/vv77/PkUfChReGeWEuuECt\ndJEcklK4m1k/4H7C0MkJ7n5PJfuMAS4EtgPXufu76SxUamnHDvj449AKLy6G994LwxSXLoWdO/ff\nt2nTcD/T738fLroIevXSCVKRHFVtuJtZA2Ac8ANgA7DAzJ5x9+KkfS4EOrt7VzPrBTwM9K6nmgVC\nMG/dGm4wvWnTvqV8e80a+OijsF6V9u3D3C99+oTle98LAS8iOS+VlntPYIW7rwYws2lAf6A4aZ/+\nwGMA7v6mmbUws9buvjndBWcN9zBH+Z49oYsjlfXdu+Gbb0Iwp/L49ddhZEr58sUX+9Z3706tzkaN\nwlDFjh2hSxfo1i0sJ58Mhx9er1+RiEQnlXBvB6xN2l5HCPwD7bM+8dy3w/3UU0MwQngsX5K36+O1\nuh6nrGxfWJeWhu0oNW4MLVuGe4+2bh0ek5e2baFTpzBcUV0rInkn4ydUCxcv3rtekFhyVsOGoWXc\nqNH+6xW3y9cbN4aDDgo3rKjqMXn9kENC67pFi7AkrzdrBmZRfwMiUg+KioooKiqq0zHMy1umVe1g\n1hsodPd+ie3bAE8+qWpmDwNz3H16YrsYOLdit4yZuS9cGEKpPJjK1ytuZ/q16vY12z+oGzRQuIpI\nRpgZ7l6jwEml5b4A6GJmxwEbgYHAoAr7PAvcDExP/DL4osr+9u7da1KfiIjUQrXh7u6lZjYCmM2+\noZDLzGx4eNnHu/sLZvYvZraSMBTy+votW0REDqTabpm0fpiZZ/LzRETioDbdMprPXUQkhhTuIiIx\npHAXEYkhhbuISAwp3EVEYkjhLiISQwp3EZEYUriLiMSQwl1EJIYU7iIiMaRwz2F1nRJU9qfvM330\nXUZP4Z7D9A8ovfR9po++y+gp3EVEYkjhLiISQxmf8jdjHyYiEiM1nfI3o+EuIiKZoW4ZEZEYUriL\niMRQRsLdzK4wsyVmVmpmp1V4bZSZrTCzZWZ2QSbqiRMzu8PM1pnZO4mlX9Q15Roz62dmxWa23MxG\nRl1PrjOzj81skZktNLO3oq4n15jZBDPbbGaLk55raWazzewDM3vZzFpUd5xMtdzfAy4D5iY/aWYn\nAlcCJwIXAg+aWY1OGggAo939tMTyUtTF5BIzawCMA/oC3wUGmdkJ0VaV88qAAnfv4e49oy4mB00k\n/Dwmuw34m7t/B/gfYFR1B8lIuLv7B+6+AqgY3P2Bae6+x90/BlYA+mGoOf1CrL2ewAp3X+3uJcA0\nws+l1J6hLt9ac/d5wOcVnu4PTEqsTwIGVHecqP8C2gFrk7bXJ56TmhlhZu+a2aOp/HdN9lPxZ3Ad\n+hmsKwdeMbMFZnZj1MXExNHuvhnA3TcBR1f3hkbp+mQzewVonfwU4S/5d+7+XLo+Jx8d6LsFHgTu\ndHc3s/8ARgPDMl+lyF5nu/tGMzuKEPLLEq1RSZ9qx7CnLdzd/fxavG090D5p+9jEc5KkBt/tI4B+\nkdbMeqBD0rZ+BuvI3TcmHj81s5mEri+Fe91sNrPW7r7ZzNoAn1T3hii6ZZL7h58FBppZEzPrCHQB\ndHa9BhJ/0eUuB5ZEVUuOWgB0MbPjzKwJMJDwcym1YGYHm9khifXmwAXoZ7I2jG9n5XWJ9SHAM9Ud\nIG0t9wMxswHAWOBIYJaZvevuF7r7UjP7K7AUKAFucl0yW1N/NLPuhBEKHwPDoy0nt7h7qZmNAGYT\nGjsT3H1ZxGXlstbAzMRUI42Ax919dsQ15RQzmwIUAK3MbA1wB3A38ISZDQVWE0YZHvg4ylIRkfiJ\nerSMiIjUA4W7iEgMKdxFRGJI4S4iEkMKdxGRGFK4i4jEkMJdRCSGFO4iIjH0/wGmgTLYd9ne/wAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21ee4096d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "plt.plot(x, sigmoid(x), 'r', linewidth=2)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "plt.title('Logistic function')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model then becomes,\n",
    "\n",
    "$$\n",
    "p(y \\mid x, \\theta) = \\left \\{\n",
    "\\begin{array}{l l}\n",
    "  \\sigma(\\theta^\\intercal x)\n",
    "  & \\text{ if } y=1  \\\\\n",
    "  1 - \\sigma(\\theta^\\intercal x)\n",
    "  & \\text { if } y=0,\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "That is, the probability that $y = 1$ given $x$ and $\\theta$\n",
    "is $\\sigma(\\theta^\\intercal x)$. Similarly, the probability that $y=0$ given $x$ and $\\theta$ is $1-\\sigma(\\theta^\\intercal x)$. It is then possible to use the function $p$ to make classification by returning the most likely class. That is, compute both probabilities and return the most probable one. \n",
    "\n",
    "It is important that you notice this function depends heavily on $\\theta$. If we want any kind of success in classifying with $p$, we need to learn values of $\\theta$ such that $p$ has little error. \n",
    "\n",
    "<div style=\"border: 1px solid black; margin: 16px; padding: 16px; \"><b>Fact: </b>The logistic function has a very simple derivative<br><br>\n",
    "\n",
    "$$\\frac{\\partial \\sigma }{\\partial z} = (1-\\sigma(z))\\sigma(z)$$<br>\n",
    "\n",
    "This is one of our main reasons for choosing this function, it makes everything a lot easier.\n",
    "</div>\n",
    "\n",
    "## 3. A. Likelihood\n",
    "\n",
    "The dataset has the form $D = (X, Y) = \\{(x_i, y_i) \\mid i = 1, \\dots, n \\}$, where $x_i \\in \\{1\\} \\times \\mathbb{R}^d$ is the $(d+1)$-dimensional data point (with the first dimension used for the bias 1) and $y_i \\in \\{0,1\\}$ is its label. For convinience we construct the following data matrix $X$ and label vector $y$\n",
    "\n",
    "$$X=\\begin{pmatrix} \n",
    "1&- & x_1 & - \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1&- & x_n & - \\\\\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times (d+1)}\\quad\\quad \n",
    "y=\\begin{pmatrix}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\\end{pmatrix}\\in\\{0,1\\}^n$$\n",
    "\n",
    "It is important that you notice that the entries $y_i$ of $y$ are not probabilities, but instead actual realisation of events.\n",
    "\n",
    "We assume (as always) that the points in $D$ are independently sampled from the unknown input distribution.\n",
    "This means that under our model, for a fixed parameter vector $\\theta$, the likelihood of the data is precisely\n",
    "\n",
    "$$\\begin{align}\n",
    "p(D \\mid \\theta)\n",
    "&= \\prod_{(x,y)\\in D}\n",
    "  p(y \\mid x,\\theta)\\\\\n",
    "&= \\prod_{(x,y)\\in D}\n",
    "  \\sigma(\\theta^\\intercal x )^{y}\n",
    "  (1-\\sigma(\\theta^\\intercal x))^{1-y}\n",
    "  \\end{align}\n",
    "$$\n",
    "\n",
    "The first equality follows from independence. The second equality uses that $p(y\\mid x, \\theta)=\\sigma(\\theta^\\intercal x )^{y}\n",
    "  (1-\\sigma(\\theta^\\intercal x))^{1-y}$. You should convince yourself that this is true; try look at both cases $y=0$ and $y=1$. This gives a convenient way of expressing\n",
    "the probability $p(y \\mid x,\\theta)$ as a product.\n",
    "\n",
    "As derived in class the negative log likelihood (<a href=\"https://en.wikipedia.org/wiki/Cross_entropy\">cross entropy</a>) is:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathrm{NLL}(D\\mid \\theta)\n",
    "&= -\\log{p(D\\mid\\theta)}\\\\\n",
    "&=- \\sum_{i=1}^n\n",
    "y_i \\ln(\\sigma(\\theta^\\intercal x_i)) +\n",
    "(1-y_i) \\ln(1-\\sigma(\\theta^\\intercal x_i))\n",
    "\\end{align}$$\n",
    "\n",
    "Notice that one of the two terms inside the sum is zero every time because $y_i\\in\\{0, 1\\}$.\n",
    "\n",
    "\n",
    "The gradient (vector) is\n",
    "\n",
    "$$\n",
    "\\nabla \\mathrm{NLL}(D \\mid \\theta)\n",
    "= \\frac{\\partial \\mathrm{NLL}}{\\partial \\theta}\n",
    "= -X^\\intercal(Y-\\sigma(X\\theta))\n",
    "$$\n",
    "\n",
    "We define the error we want to minimize to be $E_\\textrm{in} = \\frac{1}{n} \\mathrm{NLL}$. The gradient of the error is then $\\nabla E_\\textrm{in} = \\frac{1}{n} \\nabla \\mathrm{NLL}$ which is given above. The goal is now to find a good $\\theta$ such that we minimize the in-sample error $E_{in}$. In this exercise you must do this by implementing a gradient descent algorithm as explained in the next section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. B. Gradient descent\n",
    "There a several variations of Gradient Descent. They all attempt to solve the following optimization problem: \n",
    "\n",
    "<p style=\"text-indent: 16px;\">Given a function $F(\\theta)$ find $\\theta$ that approximately minimizes $F$. </p>\n",
    "\n",
    "In some simple cases we can differentiate $F(\\theta)$ and solve $F'(\\theta)=0$ for $\\theta$. This will give us the optimal solution $\\theta^*$ which by far is better than any approximation. Unfortunately, this is often not possible for the functions we care about in this course.  \n",
    "\n",
    "All variations of <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\">Gradient Descent</a> follow the same iterative methods:\n",
    "\n",
    "1. Find initial $\\theta_0$ (could be initialized randomly)\n",
    "2. Repeat for $t=1,\\ldots,T$\n",
    "3. $\\quad\\quad$Compute the gradient $g_t=\\nabla_{\\theta} f(\\theta_t)$\n",
    "4. $\\quad\\quad$Take a step in the direction of the negative gradient $\\theta_{t+1}=\\theta_t - \\eta\\cdot \\nabla_{\\theta} f(\\theta_t)$\n",
    "\n",
    "Here $\\eta$ is called the *step size* or the *learning rate*. The size of $\\eta$ is often set by experimenting with different values in practice. \n",
    "\n",
    "<img src=\"http://i.imgur.com/uqKsueE.jpg\" style=\"width: 300px;\" />\n",
    "\n",
    "There is a similar figure in the book page 94. Too small $\\eta$ is inefficient when we are too far from the local minima. Too large $\\eta$ is undesirable when we are too close to the local minima. This problem is sometimes solved by having a decreasing step size. In this handin it will be sufficient to use a fixed step-size, but you are free to experiment with varying step size.\n",
    "\n",
    "<!--If $F$ is <a href=\"https://en.wikipedia.org/wiki/Convex_function\">convex</a> gradient descent (any theoretical guarantees). If the function is not convex we risk getting stuck at local optima. In practice this is resolved by running Gradient Descent multiple times (e.g. Neural Networks). -->\n",
    "\n",
    "\n",
    "### 3. B. a. (Batch) Gradient Descent\n",
    "The function we would like to minimize is the in-sample error $E_{in}$. That is we want to find $\\theta$ that minimize $F(\\theta)=E_{in}(\\theta)$. But the in-sample error rate is often defined as the point-wise errors of all examples $(x_i, y_i)$ in our data set. Thus, evaluating $\\nabla_\\theta F(\\theta_t)$ in step 3 of gradient descent will take $O(n)$ time where $n$ is the size of our dataset (i.e. we have to look at all training examples). This is called <b>Batch</b> Gradient Descent because the gradient $\\nabla F(\\theta_i)$ is evaluated on the entire data set. \n",
    "\n",
    "Stochastic Gradient Descent attempts to speed up this process by not looking at the entire dataset. \n",
    "\n",
    "### 3. B. b. Stochastic gradient descent\n",
    "Each iteration of Batch Gradient Descent needs to consider the entire data set to compute the gradient. This often makes every iteration very slow. **Stochastic gradient descent** (SGD) is a much faster method that uses randomization to avoid looking at all the data points in each step.\n",
    "\n",
    "In SGD, our cost function $F$ must be an average over all the points, that is, a function on the form $F(\\theta) = \\frac{1}{n} \\sum_{i = 1}^n F_i(\\theta)$. Notice that this is the case for logistic regression considered above.\n",
    "When the cost function is an average, the gradient of the cost becomes \n",
    "\n",
    "$$\\nabla F(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\nabla F_i(\\theta) = E_{i}[\\nabla F_i(\\theta)]$$\n",
    "\n",
    "In SGD we estimate $\\nabla F(\\theta)$ by picking a random data point $i$ and computing $\\nabla F_i(\\theta)$. This is an <a href=\"https://en.wikipedia.org/wiki/Bias_of_an_estimator\">unbiased estimator</a> for $\\nabla F(\\theta)$ (i.e. they have same expected value) which we can use instead of $\\nabla F(\\theta)$ at each iteration.\n",
    "\n",
    "In practice we will not sample a random $i$ at each iteration, but approximate the above method by running multiple so-called *epochs* where the data is processed in a random order. At the beginning of each epoch, we output the current true value of $F(\\theta)$, and we <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.shuffle.html\">shuffle</a> the data set before processing each data point as above.\n",
    "\n",
    "### 3. B. c. Minibatch stochastic gradient descent\n",
    "\n",
    "In SGD described above, the estimate of $\\nabla F(\\theta)$ has a high <a href=\"https://en.wikipedia.org/wiki/Variance\">variance</a> since it only considers a single data point $i$. The idea of minibatch SGD is to sample more than one point from the dataset when estimating the gradient. The idea is that this decreases the variance while maintaining the advantage of each iteration being fast. Namely, we pick a minibatch size $k$ (typically between 10 and 50) and sample $i_1, i_2, \\dots, i_k$, and then we compute the better estimator (in terms of variance) $\\frac{1}{k} \\sum_{j=1}^k \\nabla F_{i_j}(\\theta)$ for $\\nabla F(\\theta)$. \n",
    "\n",
    "As for SGD, in practice we will approximate this by running epochs where the data is shuffled at the beginnning of each epoch, after which all datapoints are processed $k$ at a time in the shuffled order. \n",
    "\n",
    "As explained in the book page 93 it is required that $F$ be twice differentiable. Does it matter if $F$ is <a href=\"https://en.wikipedia.org/wiki/Convex_function\">convex</a> when we run Gradient Descent? (HINT: re-read page 93) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "<ol>\n",
    "<li>In the file \"*logistic_regression.py*\" you should complete the following methods \n",
    "<ol>\n",
    "<li>logistic</li> \n",
    "<li>log_cost</li>\n",
    "<li>batch_grad_descent</li>\n",
    "<li>mini_batch_grad_descent</li>\n",
    "</ol>\n",
    "You can test your implementation by running \"*python logistic_regression.py*\". This is a small non-exhaustive test. Consider writing your own test cases.  \n",
    "</li>\n",
    "\n",
    "<br><li>In \"*h1_classifiers.py*\" there is a class 'LogisticClassifierTwoClass'. This is a binary classifier, that is, it classifies between two classes like determining if an image is 2 vs 7. You should \n",
    "<ol>\n",
    "<li>Finish the implementation of all methods of LogisticClassifierTwoClass.</li>\n",
    "<li>Finish the implementation of the function 'model_accuracy'. </li>\n",
    "<li>At this point you will have working code that can perform binary classification. Test this by running \"*python model_stats.py -log*\" (this might take a few minutes). This outputs 3 csv files **results_2_vs_7.csv**, **pairwise_scores_train.csv** and **pairwise_scores_test.csv** you should include in your report. \n",
    " <li>The parameter vector of your 2 vs 7 classifier is visualized as a picture **logistic_regreesion_two_class_parameter_plot_1_128.png**. Does it look like something you recognize? Why is that? </li>\n",
    "</ol>\n",
    "</li>\n",
    "\n",
    "<br><li>In \"*h1_classifiers.py*\" there is another class LogisticClassifier. This is the final classifier that should learn how to classify inputs from all 10 classes (it is a full digit OCR classifier). \n",
    "<ol>\n",
    "<li>Implement the lacking methods of the LogisticClassifier class by using the <a href=\"https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest\">all-vs-one</a> technique.  \n",
    "</li> \n",
    "<li>Re-run \"*python model_stats.py -log*\". The results are stored in files **logistic_regression_classification_report.csv**, **logistic_regression_confusion_matrix.csv**, **logistic_regression_accuracy.csv**, and the visualization of the learned parameter vectors in **logistic_regression_two_class_parameter_plot_1_128.png** which again must be included in your report.</li>\n",
    "</ol>\n",
    "</li>\n",
    "\n",
    "<br><li>\n",
    "Improving the final classifier:\n",
    "<ol>\n",
    "<li>Add regularization to the cost function *log_cost* in \"*logistic_regression.py*\". Remember not to apply regularization to the bias parameter. </li>\n",
    "<li>Implement validation by completing *run_validation* in \"*h1_classifiers.py*\" </li>\n",
    "<li>Find the best parameter settings for regularization by specfying different *reg* values in *best_model* in \"*model_stats.py*\" and run \"*python model_stats.py -log_val*\". This outputs the results\n",
    " **logistic_regression_validation_scores.csv**, **logistic_regression_best_result.csv** also to be included in your report.<br><br>\n",
    "\n",
    "HINT: values are often exponentially decreasing like [0.3\\*\\*i for i in range(10)]. </li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "You can now upload your score to the highscore list by running \"*python upload_score.py STUDY_ID GROUPNAME*\".  \n",
    "\n",
    "### Report\n",
    "Add a section called \"PART I: Logistic Regression\" with subsections \"Code\" and \"Theory\" to your report. In the code subsection you should have the following subsubsections\n",
    "<ol>\n",
    "<li>Summary: Briefly explain the results you achieve and any issues you encountered.</li>\n",
    "<li>Results and plots: As stated above you should include all the results and plots your algorithms produced. Comment anything you believe sticks out, but use only few lines per plot. </li>\n",
    "<li>Weight Parameter: Specifically for weight parameters (the <b>.png</b> images), you should write three lines describing why you think they look the way they do. </li>\n",
    "<li>Best Parameters: \n",
    "State the regularization parameters you used for the best classifier you found and state the accuracy on the test set for the best model you trained.</li>\n",
    "</ol>\n",
    "\n",
    "Furthermore you must answer the following theoretical questions\n",
    "(although the bonus question is optional).\n",
    "\n",
    "### Theoretical Questions\n",
    "<ol>\n",
    "<li>What is the running time of your batch gradient descent and your mini gradient descent algorithm?<br><br>\n",
    "  \n",
    "  The parameters:<br>\n",
    "  * n*: number of training samples<br>\n",
    "  * d*: dimensionality of training samples<br>\n",
    "  * epochs*: number of epochs run<br>\n",
    "  * mini_batch_size*: batch_size for mini_batch_gradient_descent<br><br>\n",
    "  \n",
    "  Write both the time to compute the cost and the gradient for log_cost</li>\n",
    "\n",
    "<br><li> What is the running time of your implementation of logistic regression for two classes? For K classes? </li>\n",
    "\n",
    "<br><li>Sanity Check:<br>\n",
    "  What happens if we randomly permute the pixels in each image (with the same permutation) before we train the classifier? Will we get a classifier that is better, worse, or the same? Give a short explanation (at most three sentences)<br><br>\n",
    "  HINT: The location of pixels relative to each other seem to hold some kind of information. Does a random permutation of all pixels position affect this locality? Does the model we use exploit pixel locality?</li>\n",
    "\n",
    "<br><li>(BONUS) Linear Separable:<br>\n",
    "  If the data is linearly separable, what happens\n",
    "  to weights when we implement logistic regression with gradient\n",
    "  descent? That is, how do the weights that minimize the negative log likelihood look like?<br><br>\n",
    "  You may assume that we have full precision (that is, ignore floating point errors) and we can run gradient descent as long as we want (i. e. what happens with the weights in the limit). <br>\n",
    "\n",
    "  Do they converge to some fixed number (fluctuate around it) or do they\n",
    "  keep increasing in magnitude (absolute value)?<br>\n",
    "\n",
    "  Give a short explanation for your answer (you may include math).(at most 5 lines)<br>\n",
    "\n",
    "  What happens if we add regularization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Multinomial/Softmax  Regression\n",
    "\n",
    "In this exercise we generalize logistic regression to handle $K$\n",
    "classes instead of 2. This is known as Multinomial regression or Softmax.\n",
    "We will use the exact same approach as for logistic\n",
    "regression, but it becomes a little more technical due to the extra\n",
    "classes.\n",
    "\n",
    "With Softmax we are classifying into $K$ classes (instead\n",
    "of 2 for logistic regression). For this exercise we encode the target values, $y$, as a vector of\n",
    "length $K$ with all zeros except one which corresponds to the class (one-in-K encoding).\n",
    "If an example belong to class 3 and there are five classes then\n",
    "$y = [0,0,1,0,0]^\\intercal = e_3$.\n",
    "\n",
    "So $Y$ is now a matrix of size $n \\times K$, and the data matrix $X$ is unchanged.\n",
    "\n",
    "$$X=\\begin{pmatrix} \n",
    "1&- & x_1^T & - \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1&- & x_n^T & - \\\\\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times (d+1)}\\quad\\quad \n",
    "y=\\begin{pmatrix}\n",
    "- & y_1^T & -\\\\\n",
    "- & \\vdots &- \\\\\n",
    "- & y_n^T & -\\end{pmatrix}\\in\\{0,1\\}^{n\\times K}$$\n",
    "\n",
    "We can no longer use the logistic function as the probability function.\n",
    "Instead we use a generalization which is the *softmax* function.\n",
    "Softmax takes as input a vector of length $K$\n",
    "and outputs another vector of the same length $K$,\n",
    "that is a mapping from the $K$ input numbers into $K$\n",
    "*probabilities*, e.g. they sum to one.  Softmax is defined as\n",
    "\n",
    "$$\n",
    "\\textrm{softmax}(x)_j =\n",
    "\\frac{e^{x_j}}\n",
    "{\\sum_{i=1}^K e^{x_i}}\\quad\n",
    "\\textrm{ for }\\quad j = 1, \\dots, K.\n",
    "$$\n",
    "\n",
    "Notice that the denominator acts as a normalization term that ensures\n",
    "that the probabilities sum to one. Again we get nice derivatives,\n",
    "\n",
    "$$\n",
    "\\frac\n",
    "{\\partial \\;\\textrm{softmax}(x)_i}\n",
    "{\\partial x_j} =\n",
    "(\\delta_{i,j} - \\textrm{softmax}(x)_j)\n",
    "\\textrm{softmax}(x)_i\\quad\\quad\\text{where}\\quad\\quad\n",
    "\\delta_{ij}=\\begin{cases}1 &\\text{if }i=j\\\\\n",
    "0 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Exactly as before this is a linear model for each class. The parameter set\n",
    "$\\theta$ is represented as a $(d+1) \\times K$ matrix giving $d+1$\n",
    "parameters (+1 for the bias) for each of the $K$ classes (parameters\n",
    "for class $c$ is column $c$), meaning that\n",
    "$\\theta=[\\theta_1,\\ldots,\\theta_K]$. We get\n",
    "\n",
    "$$\n",
    "p(y \\mid x,\\theta) =\n",
    "\\textrm{softmax}(\\theta^\\intercal x) =\n",
    " \\left \\{\n",
    "\\begin{array}{l l}\n",
    " \\textrm{softmax}(\\theta^\\intercal x)_1 & \\text{ if } y = e_1,  \\\\\n",
    " \\vdots & \\\\\n",
    " \\textrm{softmax}(\\theta^\\intercal x)_K & \\text { if } y = e_K.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Think of the probability distribution over $y$ as throwing a $K$-sided die\n",
    "where the likelihood of landing on each of the $K$ sides is stored in the\n",
    "vector $\\textrm{softmax}(\\theta^\\intercal x)$ (which is a vector of length $K$) and the\n",
    "probability of landing on side $i$ is $\\textrm{softmax}(\\theta^\\intercal x)_i$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-48bdea75ceac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msoftmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'softmax of the ones vector: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'That seems resonable, right?'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 1, 1])\n",
    "softmax = np.exp(x)/np.sum(np.exp(x))\n",
    "print('softmax of the ones vector: ', softmax)\n",
    "print('That seems resonable, right?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for logistic regression we compute the likelihood of the data given a fixed matrix of parameters.\n",
    "We use the notation $[z]$ for the indicator function e.g. $[z]$ is one if $z$ is true and zero otherwise.\n",
    "\n",
    "$$\n",
    "P(D \\mid \\theta) =\n",
    "\\prod_{(x,y)\\in D}\n",
    "\\prod_{j=1}^K\n",
    "\\textrm{softmax}(\\theta^\\intercal x)_j^{[y_j=1]}\n",
    "=\n",
    "\\prod_{(x,y)\\in D}\n",
    "y^\\intercal\n",
    "\\textrm{softmax}(\\theta^\\intercal x)\n",
    ".\n",
    "$$\n",
    "\n",
    "This way of expressing is the same as we did for logistic regression.\n",
    "The product over the $K$ classes will have one element\n",
    "that is not one namely the $y_j$'th element ($y$ is a\n",
    "vector of $K-1$ zeros and a one). The remaining probabilities are\n",
    "raised to a power of zero and has the value one.\n",
    "\n",
    "For convenience we minimize the negative log likelihood of the data\n",
    "instead of maximizing the likelihood of the data and get a pointwise sum.\n",
    "\n",
    "$$\n",
    "\\begin{align}\\textrm{NLL}(D\\mid \\theta) &=\n",
    "-\\sum_{(x,y)\\in D}\n",
    "\\sum_{j=1}^K\n",
    "[y_j=1]\n",
    "\\ln (\\textrm{softmax}(\\theta^\\intercal x)_j)\n",
    "\\\\\n",
    "&=-\\sum_{(x,y)\\in D}\n",
    "y^\\intercal\n",
    "\\ln (\\textrm{softmax}(\\theta^\\intercal x))\n",
    ".\n",
    "\\end{align}\n",
    "$$\n",
    "Notice again that inside the last summation only one value will be nonzero. For a particular data point (x, y) where $y=e_j$ let $z = \\theta^\\intercal x$ be the input to softmax then the cost for that point is just\n",
    "$$\n",
    "- \\ln \\textrm{softmax}(z)_j = \\ln \\left( \\frac{e^{z_j}}{\\sum_{i=1}^d e^{z_i}}\\right) = - (z_j - \\ln \\sum_{i=1}^d e^{z_i})\n",
    "$$\n",
    "\n",
    "\n",
    "Again we define $E_\\textrm{in} = \\frac{1}{|D|} \\textrm{NLL}$ and as for logistic regression this is a convex function which cannot be solved for a zero analytically.  To apply gradient descent as before all you really need is the\n",
    "gradient of the negative log likelihood function.  This gradient is a\n",
    "*simple* generalization of the one used in logistic\n",
    "regression. There is a set of parameters for each of $K$ classes, $\\theta_j$ for\n",
    "$j=1,\\ldots,K$ (the $j$'th column in the parameter matrix $\\theta$) that must be learned.\n",
    "Luckily some nice people tell you what it is:\n",
    "$$\n",
    "\\nabla \\textrm{NLL}(\\theta) =\n",
    "-X^\\intercal\n",
    "(Y - \\textrm{softmax}(X\\theta)),\n",
    "$$\n",
    "\n",
    "where softmax is taken on each row of the matrix (that is, $X \\theta$ is an\n",
    "$n \\times K$ matrix and softmax is computed for each training case over\n",
    "the $K$ classes).\n",
    "\n",
    "You should, of course, verify this yourself but you do not have to\n",
    "prove it. \n",
    "\n",
    "\n",
    "\n",
    "## Numerical Issues with Softmax\n",
    "There are some numerical issues with the softmax function\n",
    "\n",
    "$$\n",
    "\\textrm{softmax}(x)_j = \\frac{e^{x_j}}{\\sum_{i=1}^K e^{x_i}} \\textrm{ for } j=1,\\ldots,K.\n",
    "$$\n",
    "\n",
    "This is because this is a sum of exponentials (before taking logs again),\n",
    "and exponentiation of numbers tend to make them very large giving numerical problems.\n",
    "Let's look at the function for a fixed $j$,\n",
    "$\\frac{e^{x_j}}{\\sum_{i=1}^K e^{x_i}}$.\n",
    "Since the logarithm and the exponential function are each other's inverse,\n",
    "we may write it as\n",
    "$$\n",
    "e^{\\textstyle x_j - \\ln(\\sum_{i=1}^K e^{x_i})}\n",
    "$$\n",
    "\n",
    "The problematic part is the logarithm of the sum of exponentials.\n",
    "However, we can move $e^c$ for any constant $c$ outside the sum easily, that is,\n",
    "$$\n",
    "\\ln\\left(\\sum_i e^{x_i}\\right) =\n",
    "\\ln\\left(e^c \\sum_i e^{x_i-c}\\right) =\n",
    "c + \\ln\\left(\\sum_i e^{x_i -c}\\right).\n",
    "$$\n",
    "\n",
    "We need to find a good $c$, and we choose $c = \\max_i x_i$ since\n",
    "$e^{x_i}$ is the dominant term in the sum. We are less concerned with\n",
    "values being inadvertently rounded to zero since that does not\n",
    "change the value of the sum significantly.\n",
    "\n",
    "With this in place you should be able to make a gradient descent\n",
    "implementation for softmax regression, like you did for logistic\n",
    "regression.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deliverables\n",
    "<ol>\n",
    "\n",
    "<li>In \"*softmax.py*\" you must complete the implementation of \n",
    "<ol>\n",
    "<li>soft_cost</li>\n",
    "<li>batch_grad_descent</li>\n",
    "<li>mini_batch_grad_descent</li>\n",
    "</ol>\n",
    "Test your implementation with \"*python softmax.py\"*. You should then complete the implementation of the class SoftmaxClassifier in \"*h1_classifiers.py*\"</li>\n",
    "\n",
    "<br><li>Run \"*python model_stats.py -soft*\" and see the results of the full classifier using the provided parameters. The results are stored in files **softmax_classification_report.csv**, **softmax_confusion_matrix.csv**, **softmax_accuracy.csv**, which again must be included in your report as well as the visualization of learned softmax parameters in **softmax_parameter_plot_1_128.png**</li>\n",
    "\n",
    "<br><li>Add regularization to the cost function, **soft_cost** in softmax.py  (remember not to apply regularization to the bias parameters)</li>\n",
    "\n",
    "<br><li>Find the best parameter settings for regularization by specfying different *reg* values in *best_model* in \"model_stats.py\" and run \"python model_stats.py -soft_val\". This outputs the results\n",
    " **softmax_validation_scores.csv**, **softmax_best_result.csv** also to be included in your report.\n",
    "\n",
    " HINT: values are often exponentially decreasing like [0.3\\*\\*i for i in range(10)]. </li>\n",
    " </ol>\n",
    "\n",
    "\n",
    "### Report\n",
    "Add a section \"Part II: Softmax\" with subsections \"code\" and \"theory\" to your report. In the \"code\" subsection <b>you should do the same 4 point as you did for logistic regression</b>. There is a single theory question specified in the next section. \n",
    "\n",
    "Add a section called \"Part III: Softmax vs Logistic Regression\". Using at most three lines: what is best softmax or one-vs-all with logistic regression? Why?\n",
    "\n",
    "### Theoretical Question(s):\n",
    "Assume that you use your softmax implementation on a problem with $K$ classes with n,d, epochs, batch_size defined as for logistic_regression.\n",
    "* What is the running time of your softmax implementation i.e how long does soft_cost takes to compute the cost and the gradient.\n",
    "\n",
    "\n",
    "# Uploading to Black Board\n",
    "Running\n",
    "\n",
    "    python3 zip_code_results.py\n",
    "\n",
    "Gives you a file \"handin1_upload_files.zip\" with the code files and the automatically generated results and plots.\n",
    "Note that this command overwrites the file \"handin1_upload_files.zip\".\n",
    "\n",
    "Upload this to blackboard together with your report 'article'.pdf \n",
    "Remeber to put your names and student ids inside the pdf report!!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
